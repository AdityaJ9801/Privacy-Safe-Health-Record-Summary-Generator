# üè• Medical Report Analysis API

> **Production-grade FastAPI application for medical report summarization and analysis using Google's MedGemma GGUF model with automatic GPU/CPU detection.**

Built for doctors and healthcare professionals to efficiently analyze medical reports, extract insights, and generate summaries.

---

## üìã Table of Contents

- [Features](#features)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [Configuration](#configuration)
- [Running the API](#running-the-api)
- [API Endpoints](#api-endpoints)
- [Usage Examples](#usage-examples)
- [Docker Deployment](#docker-deployment)
- [Project Structure](#project-structure)
- [Technology Stack](#technology-stack)
- [Testing](#testing)
- [Troubleshooting](#troubleshooting)
- [Production Deployment](#production-deployment)
- [Performance](#performance)
- [Deployment Checklist](#deployment-checklist)

---

## ‚ú® Features

### Core Capabilities
- ‚úÖ **Medical Report Summarization** - Generate concise summaries from lengthy medical reports
- ‚úÖ **Question Answering** - Ask specific questions about medical reports and get accurate answers
- ‚úÖ **Real-time Streaming** - Token-by-token streaming for better UX (like ChatGPT)
- ‚úÖ **Document Processing** - Support for PDF and text documents with automatic text extraction
- ‚úÖ **Image Processing** - OCR support for medical images (scans, X-rays, handwritten notes)
- ‚úÖ **RAG Support** - Handle large documents efficiently with vector-based retrieval
- ‚úÖ **Multi-modal Support** - Process both text and image inputs

### Production Features
- ‚úÖ **Automatic GPU/CPU Detection** - Intelligently detects and uses GPU if available, falls back to CPU
- ‚úÖ **GGUF Model Format** - Efficient quantized models (~2.5GB vs ~8.6GB)
- ‚úÖ **Local-First Loading** - Caches model locally, no re-download needed
- ‚úÖ **Modular Architecture** - Clean, maintainable, and scalable code structure
- ‚úÖ **Type Safety** - Full type hints with Pydantic models
- ‚úÖ **Error Handling** - Comprehensive exception handling and logging
- ‚úÖ **API Documentation** - Interactive Swagger UI and ReDoc
- ‚úÖ **Docker Support** - Easy deployment with Docker and Docker Compose
- ‚úÖ **Comprehensive Testing** - Full test suite with deployment validation
- ‚úÖ **Logging** - Structured logging with rotation and levels
- ‚úÖ **Configuration** - Environment-based configuration management

---

## üöÄ Quick Start

Get up and running in 3 minutes!

### Prerequisites
- Python 3.10+ installed
- (Optional) NVIDIA GPU with CUDA for faster inference
- At least 4GB RAM
- At least 5GB disk space (for model cache)

### Installation

**Option 1: Interactive Installer (Recommended)**
```bash
python install.py
```
This will:
- ‚úÖ Auto-detect GPU
- ‚úÖ Install correct dependencies
- ‚úÖ Guide you through setup

**Option 2: Manual Installation**

**CPU Only:**
```bash
pip install -r requirements.txt
```

**GPU Support (CUDA 12.1):**
```bash
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
pip install -r requirements.txt
```

**GPU Support (CUDA 11.8):**
```bash
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu118
pip install -r requirements.txt
```

### Run the API

```bash
python run.py
```

The API will:
- ‚úÖ Auto-detect GPU/CPU
- ‚úÖ Download model on first run (~2.5GB)
- ‚úÖ Start at `http://localhost:8000`

### Test the API

```bash
# Run comprehensive test suite
python test_sample.py

# Or check health manually
curl http://localhost:8000/api/v1/health

# Access interactive docs
# Open browser: http://localhost:8000/docs
```

---

## üì¶ Installation

### System Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **Python** | 3.10+ | 3.11+ |
| **RAM** | 4GB | 8GB+ |
| **Disk Space** | 5GB | 10GB+ |
| **GPU (Optional)** | - | NVIDIA with CUDA 11.8+ |

### Installation Options

#### Option A: Interactive Installer (Easiest)
```bash
python install.py
```
- Automatically detects GPU
- Installs correct dependencies
- Provides setup guidance

#### Option B: CPU-Only Installation
```bash
pip install -r requirements-minimal.txt
```
- Quick installation
- Works on any system
- Uses CPU for inference

#### Option C: GPU Installation (Faster Performance)

**For CUDA 12.1:**
```bash
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
pip install -r requirements-minimal.txt
```

**For CUDA 11.8:**
```bash
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu118
pip install -r requirements-minimal.txt
```

**Verify GPU Detection:**
```bash
nvidia-smi  # Should show your GPU
```

### First Run

On first run, the system will:
1. ‚úÖ Auto-detect GPU (if available) or use CPU
2. ‚úÖ Download MedGemma GGUF model (~2.5GB)
3. ‚úÖ Cache model locally (no re-download needed)
4. ‚úÖ Start API server

---

## ‚öôÔ∏è Configuration

### Environment Variables (Optional)

The system works with **zero configuration** - it auto-detects everything!

However, you can customize by creating a `.env` file:

```env
# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# Model Configuration (Auto-detected by default)
MODEL_DEVICE=cuda              # Auto-detected: cuda or cpu
MAX_MODEL_LENGTH=2048
MODEL_CACHE_DIR=./models

# Document Processing
MAX_FILE_SIZE_MB=50

# Logging
LOG_LEVEL=INFO                 # DEBUG, INFO, WARNING, ERROR
LOG_FILE=./logs/app.log

# Security (Production)
API_KEY_ENABLED=false
CORS_ORIGINS=*
```

### Configuration Notes

**Device Detection (Automatic):**
- System automatically detects CUDA GPU via `nvidia-smi`
- Falls back to CPU if GPU not available
- No manual configuration needed!

**Model Information:**
- **Repository:** `unsloth/medgemma-1.5-4b-it-GGUF`
- **File:** `medgemma-1.5-4b-it-Q4_K_M.gguf`
- **Size:** ~2.5GB (quantized)
- **Format:** GGUF (optimized for llama.cpp)

**For Production:**
```env
LOG_LEVEL=INFO
API_KEY_ENABLED=true
API_KEY=your-secure-key-here
CORS_ORIGINS=https://yourdomain.com
```

---

## üèÉ Running the API

### Development Mode

```bash
# Using the run script
python run.py

# Or with uvicorn directly
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Production Mode

```bash
# With multiple workers
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4

# Or use the run script (configured via .env)
python run.py
```

### Access the API

- **API Base URL**: http://localhost:8000
- **Interactive Docs (Swagger UI)**: http://localhost:8000/docs
- **Alternative Docs (ReDoc)**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/health

---

## üîå API Endpoints

### 1. Health Check

**Endpoint:** `GET /health`

**Description:** Check API health and model status

**Example:**
```bash
curl http://localhost:8000/health
```

**Response:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "version": "0.1.0",
  "ml_available": true,
  "device": "cuda"
}
```

**Device Field:**
- `"cuda"` - GPU acceleration active
- `"cpu"` - Running on CPU
- `"not_loaded"` - Model not yet loaded

---

### 2. Text Summarization

**Endpoint:** `POST /api/v1/summarize`

**Description:** Generate summary from medical report text

**Request:**
```bash
curl -X POST "http://localhost:8000/api/v1/summarize" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Patient presents with fever and cough. Temperature 101¬∞F. Prescribed antibiotics.",
    "max_length": 512,
    "temperature": 0.7
  }'
```

**Parameters:**
- `text` (required): Medical report text to summarize
- `max_length` (optional): Maximum length of summary (50-2048, default: 512)
- `temperature` (optional): Sampling temperature (0.1-1.0, default: 0.7)

**Response:**
```json
{
  "summary": "Patient has fever (101¬∞F) and cough. Antibiotics prescribed.",
  "input_length": 75,
  "summary_length": 52
}
```

---

### 3. Streaming Summarization ‚ö° NEW

**Endpoint:** `POST /api/v1/summarize/stream`

**Description:** Generate summary with real-time token streaming (like ChatGPT)

**Request:**
```bash
curl -X POST "http://localhost:8000/api/v1/summarize/stream" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Patient presents with fever and cough. Temperature 101¬∞F.",
    "temperature": 0.7
  }'
```

**Response Format:** Server-Sent Events (SSE)
```
data: Patient
data:  has
data:  fever
data:  (101¬∞F)
data:  and
data:  cough.
data: [DONE]
```

**Python Example:**
```python
import requests

response = requests.post(
    "http://localhost:8000/api/v1/summarize/stream",
    json={"text": "Patient has fever and cough.", "temperature": 0.7},
    stream=True
)

for line in response.iter_lines():
    if line:
        token = line.decode('utf-8')[6:]  # Remove 'data: ' prefix
        if token == "[DONE]":
            break
        print(token, end='', flush=True)
```

---

### 4. Question Answering

**Endpoint:** `POST /api/v1/analyze`

**Description:** Answer questions about medical reports

**Request:**
```bash
curl -X POST "http://localhost:8000/api/v1/analyze" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Patient presents with fever and cough. Temperature 101¬∞F.",
    "question": "What is the patient'\''s temperature?",
    "temperature": 0.7
  }'
```

**Response:**
```json
{
  "answer": "The patient's temperature is 101¬∞F.",
  "question": "What is the patient's temperature?",
  "confidence": 0.95
}
```

---

### 5. Streaming Question Answering ‚ö° NEW

**Endpoint:** `POST /api/v1/analyze/stream`

**Description:** Answer questions with real-time token streaming

**Request:**
```bash
curl -X POST "http://localhost:8000/api/v1/analyze/stream" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Patient presents with fever and cough. Temperature 101¬∞F.",
    "question": "What is the patient'\''s temperature?"
  }'
```

**Response Format:** Server-Sent Events (SSE)
```
data: The
data:  patient's
data:  temperature
data:  is
data:  101¬∞F.
data: [DONE]
```

---

### 6. Document Upload

**Endpoint:** `POST /api/v1/upload/document`

**Description:** Upload and process PDF or text documents

**Request:**
```bash
curl -X POST "http://localhost:8000/api/v1/upload/document" \
  -F "file=@medical_report.pdf"
```

**Supported Formats:** PDF, TXT

**Response:**
```json
{
  "processed": true,
  "filename": "medical_report.pdf",
  "text_length": 1523,
  "message": "Document processed and added to vector store"
}
```

---

### 7. Image Upload

**Endpoint:** `POST /api/v1/upload/image`

**Description:** Upload and process medical images with OCR

**Note:** MedGemma is a text-only model. OCR (Tesseract) is needed to extract text from images first.

**Request:**
```bash
curl -X POST "http://localhost:8000/api/v1/upload/image" \
  -F "file=@xray_report.jpg"
```

**Supported Formats:** JPG, JPEG, PNG, TIFF

**Response:**
```json
{
  "processed": true,
  "filename": "xray_report.jpg",
  "extracted_text": "X-Ray Report: No abnormalities detected...",
  "text_length": 245
}
```

---

### 6. RAG Summarization

**Endpoint:** `POST /api/v1/rag/summarize`

**Description:** Generate summary using RAG for large documents

**Request:**
```bash
curl -X POST "http://localhost:8000/api/v1/rag/summarize" \
  -H "Content-Type: application/json" \
  -d '{
    "max_length": 512,
    "temperature": 0.7,
    "top_k": 5
  }'
```

**Response:**
```json
{
  "summary": "Comprehensive summary of all documents in vector store...",
  "chunks_used": 5,
  "summary_length": 487
}
```

---

### 7. RAG Question Answering

**Endpoint:** `POST /api/v1/rag/question`

**Description:** Answer questions using RAG retrieval

**Request:**
```bash
curl -X POST "http://localhost:8000/api/v1/rag/question" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What are the main findings?",
    "top_k": 5,
    "temperature": 0.7
  }'
```

**Response:**
```json
{
  "answer": "The main findings include...",
  "question": "What are the main findings?",
  "relevant_chunks": ["chunk1", "chunk2"],
  "confidence": 0.92
}
```

---

## üí° Usage Examples

### Python Example - Text Summarization

```python
import requests

url = "http://localhost:8000/api/v1/summarize"
payload = {
    "text": "Patient presents with fever and cough. Temperature 101¬∞F. Prescribed antibiotics.",
    "temperature": 0.7
}

response = requests.post(url, json=payload)
print(response.json())
```

### Python Example - Question Answering

```python
import requests

url = "http://localhost:8000/api/v1/analyze"
payload = {
    "text": "Patient presents with fever and cough. Temperature 101¬∞F.",
    "question": "What is the patient's temperature?"
}

response = requests.post(url, json=payload)
print(response.json())
```

### Python Example - Document Upload

```python
import requests

url = "http://localhost:8000/api/v1/upload/document"
files = {"file": open("medical_report.pdf", "rb")}

response = requests.post(url, files=files)
print(response.json())
```

### Python Example - RAG Question

```python
import requests

# First upload a document
url_upload = "http://localhost:8000/api/v1/upload/document"
files = {"file": open("medical_report.pdf", "rb")}
requests.post(url_upload, files=files)

# Then query it
url_query = "http://localhost:8000/api/v1/rag/question"
payload = {
    "question": "What are the main findings?",
    "top_k": 5
}

response = requests.post(url_query, json=payload)
print(response.json())
```

---

## üê≥ Docker Deployment

### Using Docker Compose (Recommended)

```bash
# Build and start
docker-compose up -d

# View logs
docker-compose logs -f

# Stop
docker-compose down
```

### Using Docker Directly

```bash
# Build image
docker build -t medical-report-api .

# Run container
docker run -d \
  -p 8000:8000 \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/vector_store:/app/vector_store \
  -v $(pwd)/logs:/app/logs \
  --env-file .env \
  --name medical-api \
  medical-report-api

# View logs
docker logs -f medical-api

# Stop container
docker stop medical-api
```

### GPU Support in Docker

Ensure NVIDIA Container Toolkit is installed:

```bash
# Install NVIDIA Container Toolkit
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# Run with GPU
docker run -d \
  --gpus all \
  -p 8000:8000 \
  --env-file .env \
  medical-report-api
```

---

## üìÅ Project Structure

```
medical-report-api/
‚îÇ
‚îú‚îÄ‚îÄ app/                          # Main application code
‚îÇ   ‚îú‚îÄ‚îÄ api/                      # API routes and endpoints
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.py            # All API endpoints
‚îÇ   ‚îú‚îÄ‚îÄ models/                   # Model management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_loader.py      # GGUF model loader with auto GPU/CPU detection
‚îÇ   ‚îú‚îÄ‚îÄ processors/               # Document/image processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py # PDF/TXT processing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ image_processor.py   # Image processing with OCR
‚îÇ   ‚îú‚îÄ‚îÄ rag/                      # RAG implementation (optional)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py      # Vector store management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag_pipeline.py      # RAG pipeline
‚îÇ   ‚îú‚îÄ‚îÄ schemas/                  # Request/response models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requests.py          # API request schemas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ responses.py         # API response schemas
‚îÇ   ‚îú‚îÄ‚îÄ utils/                    # Utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py            # Logging configuration
‚îÇ   ‚îú‚îÄ‚îÄ config.py                # Configuration management
‚îÇ   ‚îî‚îÄ‚îÄ main.py                  # FastAPI application
‚îÇ
‚îú‚îÄ‚îÄ models/                       # Model cache directory
‚îÇ   ‚îî‚îÄ‚îÄ (downloaded models)      # Auto-downloaded on first run
‚îÇ
‚îú‚îÄ‚îÄ logs/                         # Application logs
‚îÇ   ‚îî‚îÄ‚îÄ app.log                  # Main log file
‚îÇ
‚îú‚îÄ‚îÄ vector_store/                 # Vector database (optional)
‚îÇ   ‚îî‚îÄ‚îÄ (chromadb data)          # For RAG functionality
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt              # Full dependencies with comments
‚îú‚îÄ‚îÄ requirements-minimal.txt      # Minimal dependencies
‚îú‚îÄ‚îÄ install.py                    # Interactive installer
‚îú‚îÄ‚îÄ test_sample.py               # Comprehensive test suite
‚îú‚îÄ‚îÄ run.py                        # Application entry point
‚îú‚îÄ‚îÄ Dockerfile                    # Docker configuration
‚îú‚îÄ‚îÄ docker-compose.yml            # Docker Compose setup
‚îî‚îÄ‚îÄ README.md                     # This file
```

### Core Files

| File | Purpose |
|------|---------|
| `run.py` | Start the API server |
| `test_sample.py` | Run comprehensive tests |
| `install.py` | Interactive installation |
| `requirements-minimal.txt` | Essential dependencies |
| `app/models/model_loader.py` | Auto GPU/CPU detection |
| `app/api/routes.py` | All API endpoints |

---

## üõ†Ô∏è Technology Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| **Framework** | FastAPI | >=0.109.0 |
| **ML Engine** | llama-cpp-python | >=0.2.0 |
| **Model** | MedGemma 1.5 4B (GGUF) | Q4_K_M quantized |
| **Model Format** | GGUF | Optimized for llama.cpp |
| **Vector Store** | ChromaDB (optional) | >=0.4.0 |
| **Document Processing** | PyPDF | >=4.0.0 |
| **Image Processing** | Pillow | >=10.0.0 |
| **Logging** | Loguru | >=0.7.0 |
| **Configuration** | Pydantic Settings | >=2.1.0 |
| **Testing** | Pytest | >=7.4.0 |
| **Deployment** | Docker, Docker Compose | Latest |

### Why GGUF Format?

- ‚úÖ **Smaller Size:** ~2.5GB vs ~8.6GB (70% reduction)
- ‚úÖ **Faster Loading:** Optimized binary format
- ‚úÖ **Better Performance:** Efficient inference on CPU/GPU
- ‚úÖ **Lower Memory:** Quantized models use less RAM
- ‚úÖ **Cross-Platform:** Works on CPU and GPU seamlessly

---

## üß™ Testing

### Comprehensive Test Suite

Run the complete test suite to validate all functionality:

```bash
python test_sample.py
```

**This tests:**
1. ‚úÖ Server connectivity
2. ‚úÖ Health check & GPU/CPU detection
3. ‚úÖ Text summarization
4. ‚úÖ Question answering
5. ‚úÖ Document upload (PDF/TXT)
6. ‚úÖ Image upload & processing

**Output includes:**
- Pass/fail status for each test
- Performance metrics (timing)
- Device information (GPU/CPU)
- Deployment readiness assessment
- Troubleshooting suggestions

### Manual Testing

**Using Interactive Docs (Recommended):**
1. Start API: `python run.py`
2. Open browser: http://localhost:8000/docs
3. Try different endpoints
4. See real-time responses

**Using curl:**
```bash
# Health check
curl http://localhost:8000/api/v1/health

# Summarize text
curl -X POST "http://localhost:8000/api/v1/summarize" \
  -H "Content-Type: application/json" \
  -d '{"text": "Patient has fever and cough."}'
```

**Using Python:**
```python
import requests

# Test health
response = requests.get("http://localhost:8000/api/v1/health")
print(response.json())

# Test summarization
response = requests.post(
    "http://localhost:8000/api/v1/summarize",
    json={"text": "Patient presents with fever and cough."}
)
print(response.json())
```

---

## üîß Troubleshooting

### Common Issues

#### Issue: Model download is slow
**Solution:** First run downloads ~2.5GB model. This is normal. Model is cached locally for future use.

#### Issue: GPU not detected
**Check:**
```bash
nvidia-smi  # Should show your GPU
```
**Solution:** System automatically falls back to CPU. No action needed!

#### Issue: Out of memory
**Solutions:**
- System uses quantized model (Q4_K_M) - already optimized
- Reduce context: `MAX_MODEL_LENGTH=1024` in .env
- Close other applications

#### Issue: Port 8000 already in use
**Solutions:**
```bash
# Option 1: Change port in .env
API_PORT=8001

# Option 2: Kill process (Windows)
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# Option 2: Kill process (Linux/Mac)
lsof -ti:8000 | xargs kill -9
```

#### Issue: llama-cpp-python installation fails
**Solutions:**
```bash
# Update pip first
pip install --upgrade pip

# Try CPU version
pip install llama-cpp-python

# For GPU, specify CUDA version explicitly
pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121
```

### Debug Mode

Enable detailed logging:

```env
LOG_LEVEL=DEBUG
```

View logs:
```bash
# Windows
type logs\app.log

# Linux/Mac
tail -f logs/app.log
```

### Health Check

Verify system status:
```bash
curl http://localhost:8000/api/v1/health
```

Check for:
- `"model_loaded": true` - Model is ready
- `"ml_available": true` - ML dependencies installed
- `"device": "cuda"` or `"cpu"` - Device being used

---

## üöÄ Production Deployment

### Prerequisites

- Production server (Linux recommended)
- Python 3.9+
- (Optional) GPU with CUDA support
- Reverse proxy (Nginx/Apache)
- SSL certificate

### Deployment Steps

1. **Setup server**
```bash
# Update system
sudo apt update && sudo apt upgrade -y

# Install Python and dependencies
sudo apt install python3.9 python3-pip python3-venv -y
```

2. **Clone and setup**
```bash
git clone <repository-url>
cd hack-nagpur
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

3. **Configure for production**
```env
# .env
API_HOST=127.0.0.1
API_PORT=8000
API_WORKERS=4
API_RELOAD=false
LOG_LEVEL=INFO
API_KEY_ENABLED=true
API_KEY=your-secure-api-key
CORS_ORIGINS=https://yourdomain.com
```

4. **Run with systemd**

Create `/etc/systemd/system/medical-api.service`:

```ini
[Unit]
Description=Medical Report Analysis API
After=network.target

[Service]
Type=simple
User=www-data
WorkingDirectory=/path/to/hack-nagpur
Environment="PATH=/path/to/hack-nagpur/venv/bin"
ExecStart=/path/to/hack-nagpur/venv/bin/python run.py
Restart=always

[Install]
WantedBy=multi-user.target
```

Enable and start:

```bash
sudo systemctl enable medical-api
sudo systemctl start medical-api
sudo systemctl status medical-api
```

5. **Setup Nginx reverse proxy**

```nginx
server {
    listen 80;
    server_name yourdomain.com;

    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

6. **Setup SSL with Let's Encrypt**

```bash
sudo apt install certbot python3-certbot-nginx -y
sudo certbot --nginx -d yourdomain.com
```

---

## üìä Performance

### Benchmarks

| Device | Model Size | Load Time | Inference Speed | Memory Usage |
|--------|-----------|-----------|-----------------|--------------|
| **GPU (CUDA)** | 2.5GB | ~10s | ‚ö° Fast (~1-2s) | ~3GB VRAM |
| **CPU** | 2.5GB | ~15s | üíª Medium (~3-5s) | ~4GB RAM |

### Performance Tips

**Automatic Optimization:**
- ‚úÖ System auto-detects GPU and uses it
- ‚úÖ GGUF format is pre-optimized
- ‚úÖ Q4_K_M quantization balances speed/quality

**Manual Tuning:**
```env
# Reduce context for faster inference
MAX_MODEL_LENGTH=1024

# Use multiple workers for concurrent requests
API_WORKERS=4
```

**Scaling:**
- Deploy multiple instances behind load balancer
- Use Redis for caching frequent requests
- Enable CDN for static assets

---

## üéØ Use Cases

1. **Medical Report Summarization** - Quickly summarize lengthy patient reports
2. **Clinical Decision Support** - Answer specific questions about patient data
3. **Medical Image Analysis** - Extract text from scanned reports
4. **Research and Analysis** - Batch process multiple reports
5. **EHR Integration** - Integrate with Electronic Health Record systems

---

## üìù License

[Your License Here]

---

## ü§ù Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make changes with tests
4. Submit pull request

---

## üìß Support

For issues and questions:
- Check this documentation
- Review troubleshooting section
- Open an issue on GitHub

---

## ‚úÖ Deployment Checklist

Before deploying to production:

- [ ] Run test suite: `python test_sample.py`
- [ ] All tests passing (6/6)
- [ ] GPU/CPU detection working
- [ ] Model downloaded and cached
- [ ] Health endpoint returns correct device
- [ ] Logs directory created
- [ ] Environment variables configured
- [ ] API key enabled (if needed)
- [ ] CORS origins set correctly
- [ ] SSL certificate configured
- [ ] Reverse proxy setup (Nginx/Apache)
- [ ] Monitoring enabled
- [ ] Backup strategy in place

---

## üéØ Quick Reference

### Essential Commands

```bash
# Install
python install.py

# Run
python run.py

# Test
python test_sample.py

# Check health
curl http://localhost:8000/api/v1/health

# View logs
tail -f logs/app.log
```

### Key URLs

- **API Base:** http://localhost:8000
- **Swagger Docs:** http://localhost:8000/docs
- **ReDoc:** http://localhost:8000/redoc
- **Health Check:** http://localhost:8000/api/v1/health

### Important Files

- `run.py` - Start server
- `test_sample.py` - Run tests
- `test_streaming.py` - Test streaming endpoints
- `install.py` - Install dependencies
- `requirements.txt` - All dependencies
- `app/models/model_loader.py` - GPU/CPU detection
- `logs/app.log` - Application logs

---

**Built with ‚ù§Ô∏è for the future of healthcare AI**

#   P r i v a c y - S a f e - H e a l t h - R e c o r d - S u m m a r y - G e n e r a t o r 
 
 